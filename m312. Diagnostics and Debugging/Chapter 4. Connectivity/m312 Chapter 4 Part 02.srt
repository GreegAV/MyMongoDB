
1
00:00:00.000 --> 00:00:00.750


2
00:00:00.750 --> 00:00:04.530
I'm going to connect to my
Vagrant box, my m312 vagrant

3
00:00:04.530 --> 00:00:08.700
environment, and I'm going
to launch a replica set

4
00:00:08.700 --> 00:00:12.660
that I'm going to call
timeouts with three nodes

5
00:00:12.660 --> 00:00:16.110
and where their data sets
will going to be specified

6
00:00:16.110 --> 00:00:18.390
on this folder that I
just created as well,

7
00:00:18.390 --> 00:00:23.034
called timeouts and
starting on port 27000.

8
00:00:23.034 --> 00:00:25.590
Once my replica
set is initiated,

9
00:00:25.590 --> 00:00:28.560
I'll connect to one of
the nodes and express

10
00:00:28.560 --> 00:00:30.200
the following command.

11
00:00:30.200 --> 00:00:33.630
Here, I'm going to insert
this document, Hello World,

12
00:00:33.630 --> 00:00:37.860
with a write concern expecting
three nodes to acknowledge this

13
00:00:37.860 --> 00:00:41.580
write within one millisecond.

14
00:00:41.580 --> 00:00:44.370
So anything that
takes more than one

15
00:00:44.370 --> 00:00:48.120
millisecond, I will come back
with an error saying hey,

16
00:00:48.120 --> 00:00:52.020
I could not confirm that three
nodes of your replica set

17
00:00:52.020 --> 00:00:56.250
were able to acknowledge the
write within one millisecond

18
00:00:56.250 --> 00:00:57.600
in total.

19
00:00:57.600 --> 00:01:00.150
So if I do so,
and since I'm very

20
00:01:00.150 --> 00:01:04.410
bullish about this
expected timeout,

21
00:01:04.410 --> 00:01:07.830
I actually get back a
write concern failed

22
00:01:07.830 --> 00:01:12.120
error saying well you
know what we tried

23
00:01:12.120 --> 00:01:16.260
but we were not able to get
the confirmation within one

24
00:01:16.260 --> 00:01:19.110
millisecond that all
three nodes actually

25
00:01:19.110 --> 00:01:22.200
got that write operation.

26
00:01:22.200 --> 00:01:26.590
Now this happens because we
just launched our replica set.

27
00:01:26.590 --> 00:01:30.570
Everything is in the same box
but once we launch and write

28
00:01:30.570 --> 00:01:35.119
something to that
newly boot up MongoD,

29
00:01:35.119 --> 00:01:36.660
it will need to do
a couple of things

30
00:01:36.660 --> 00:01:39.690
like writing into an index
or creating index files

31
00:01:39.690 --> 00:01:41.290
into the collection files.

32
00:01:41.290 --> 00:01:43.470
So those probably are
going to take more than one

33
00:01:43.470 --> 00:01:45.180
millisecond alone.

34
00:01:45.180 --> 00:01:49.900
Therefore, I can not
guarantee that that

35
00:01:49.900 --> 00:01:55.650
write was acknowledged by three
nodes within one millisecond.

36
00:01:55.650 --> 00:01:59.320
That would be pretty
awesome, wouldn't it?

37
00:01:59.320 --> 00:02:02.640
If I try the exact same
instruction, new document

38
00:02:02.640 --> 00:02:07.740
with the same set of parameters,
but after all the files have

39
00:02:07.740 --> 00:02:13.230
been initialized and all, I will
actually be able to insert it.

40
00:02:13.230 --> 00:02:15.030
Once everything is
initialized and I

41
00:02:15.030 --> 00:02:19.650
have everything up and
running, actually, the nodes

42
00:02:19.650 --> 00:02:22.470
are able to communicate
back that they acknowledged

43
00:02:22.470 --> 00:02:24.870
the writes below one
millisecond, which

44
00:02:24.870 --> 00:02:26.340
is pretty awesome.

45
00:02:26.340 --> 00:02:28.260
The important thing
here is that we

46
00:02:28.260 --> 00:02:31.320
need to make sure
we understand what's

47
00:02:31.320 --> 00:02:33.100
going on under the covers.

48
00:02:33.100 --> 00:02:37.290
As I can see here, I
attempted three inserts

49
00:02:37.290 --> 00:02:43.050
and got all of them, is
that although I was not

50
00:02:43.050 --> 00:02:47.370
able to confirm that all three
nodes were able to acknowledge

51
00:02:47.370 --> 00:02:51.600
below one millisecond,
the write is still

52
00:02:51.600 --> 00:02:53.940
acknowledged in the primary.

53
00:02:53.940 --> 00:02:57.990
We will always notify you
back to the application saying

54
00:02:57.990 --> 00:03:02.280
what you expected was
not able to be met.

55
00:03:02.280 --> 00:03:05.790
But, that doesn't mean that
the write will be rolled back

56
00:03:05.790 --> 00:03:09.720
or will not succeed on
the primary nodes or even

57
00:03:09.720 --> 00:03:11.760
the secondary
nodes, it just means

58
00:03:11.760 --> 00:03:16.230
it doesn't succeed within the
time I'm expecting the system

59
00:03:16.230 --> 00:03:17.910
to comply with.

60
00:03:17.910 --> 00:03:21.180
So, if by anytime
your developers

61
00:03:21.180 --> 00:03:24.930
start seeing messages
like this, make

62
00:03:24.930 --> 00:03:28.470
sure they realize
what is going on

63
00:03:28.470 --> 00:03:32.910
and be assuring them that
the data is safely stored

64
00:03:32.910 --> 00:03:37.740
in the primaries at least, just
that their expectations are not

65
00:03:37.740 --> 00:03:40.150
able to be met.

66
00:03:40.150 --> 00:03:44.430
And let's not get confused
by some other aspect

67
00:03:44.430 --> 00:03:48.000
of this write concern which is
not related with the w timeout

68
00:03:48.000 --> 00:03:49.380
itself.

69
00:03:49.380 --> 00:03:53.130
Let's go ahead and connect to
one of the secondary nodes that

70
00:03:53.130 --> 00:04:00.300
is listening on port 27001 and
let's shut down this server.

71
00:04:00.300 --> 00:04:02.460
And let me connect back
to my test database

72
00:04:02.460 --> 00:04:04.800
where I was inserting my data.

73
00:04:04.800 --> 00:04:07.260
In this case, the
message will be the same

74
00:04:07.260 --> 00:04:09.300
but even if we increase
the w timeout, just

75
00:04:09.300 --> 00:04:13.140
let's say 100
milliseconds, we will still

76
00:04:13.140 --> 00:04:18.000
get write concern failed
within the replication timeout.

77
00:04:18.000 --> 00:04:21.329
This is only due to the fact
that if one of our nodes

78
00:04:21.329 --> 00:04:24.540
is completely down,
it is impossible,

79
00:04:24.540 --> 00:04:27.720
regardless of the number
of w timeout value

80
00:04:27.720 --> 00:04:32.160
that we set, to comply back
because until it gets up

81
00:04:32.160 --> 00:04:36.150
and running probably,
it will be more time

82
00:04:36.150 --> 00:04:38.130
than 100 milliseconds.

83
00:04:38.130 --> 00:04:42.690
A slight variation will be to
request a number of nodes that

84
00:04:42.690 --> 00:04:45.240
pure and simply isn't there.

85
00:04:45.240 --> 00:04:49.770
In this case, we can not satisfy
concern will be sent back

86
00:04:49.770 --> 00:04:53.550
because there's not enough
data bearing nodes to actually

87
00:04:53.550 --> 00:04:55.400
comply with our request.

88
00:04:55.400 --> 00:04:59.060
Our data will still be stored
in the primary and all living

89
00:04:59.060 --> 00:05:03.180
secondaries but regardless
of the w timeout.

90
00:05:03.180 --> 00:05:06.090
We are expecting
more nodes than what

91
00:05:06.090 --> 00:05:09.510
our current configuration
of the replica set

92
00:05:09.510 --> 00:05:12.150
has in terms of
data bearing nodes.

93
00:05:12.150 --> 00:05:16.320
So this will not be satisfied
not because of the w timeout,

94
00:05:16.320 --> 00:05:19.740
but because we are requesting
something that's purely simply

95
00:05:19.740 --> 00:05:21.340
not possible.

96
00:05:21.340 --> 00:05:25.290
These are the kind of errors
that your developers might see

97
00:05:25.290 --> 00:05:26.730
in a production environment.

98
00:05:26.730 --> 00:05:29.070
Make sure they
understand clearly

99
00:05:29.070 --> 00:05:34.890
what those messages refer of
and how they can deal with those

100
00:05:34.890 --> 00:05:37.010
from the application side.

101
00:05:37.010 --> 00:05:39.400
Now that we've seen
what w timeout is,

102
00:05:39.400 --> 00:05:42.090
let's look into
another nice timeout

103
00:05:42.090 --> 00:05:47.570
error that can be controlled
and set by the application.