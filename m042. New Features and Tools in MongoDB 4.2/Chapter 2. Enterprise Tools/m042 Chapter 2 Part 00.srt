
1
00:00:00.000 --> 00:00:00.830


2
00:00:00.830 --> 00:00:01.490
Welcome back.

3
00:00:01.490 --> 00:00:03.073
In this lesson, we're
going to discuss

4
00:00:03.073 --> 00:00:05.240
a new feature an Atlas
called the data lake, which

5
00:00:05.240 --> 00:00:09.680
allows us to query archived data
stored in as S3 buckets MongoDB

6
00:00:09.680 --> 00:00:11.420
query language.

7
00:00:11.420 --> 00:00:14.390
So keeping all of your data
available in' database clusters

8
00:00:14.390 --> 00:00:16.430
may not be financially
responsible.

9
00:00:16.430 --> 00:00:19.370
So customers archive
data from time to time.

10
00:00:19.370 --> 00:00:21.410
Having that data
in archive format

11
00:00:21.410 --> 00:00:24.110
does not mean we won't ever
need it again because you still

12
00:00:24.110 --> 00:00:27.350
may need to access this data
for data analytics or compliance

13
00:00:27.350 --> 00:00:28.800
purposes.

14
00:00:28.800 --> 00:00:31.370
However, now you could
also use the Atlas data

15
00:00:31.370 --> 00:00:35.330
lake to query the data that
you've archived in S3 buckets.

16
00:00:35.330 --> 00:00:37.460
So you create a data lake
through the Atlas UI,

17
00:00:37.460 --> 00:00:40.590
similar to creating
an Atlas cluster.

18
00:00:40.590 --> 00:00:42.930
Your data lakes appear in
the UI at the same level

19
00:00:42.930 --> 00:00:44.035
as your Atlas clusters.

20
00:00:44.035 --> 00:00:45.660
And once you've
created your data lake,

21
00:00:45.660 --> 00:00:48.030
you can configure the data
stored in your S3 buckets

22
00:00:48.030 --> 00:00:51.990
to be exposed as MongoDB
databases or collections.

23
00:00:51.990 --> 00:00:53.700
Then you can build
and run queries

24
00:00:53.700 --> 00:00:56.130
through the Mongo
Shell, MongoDB Compass,

25
00:00:56.130 --> 00:00:58.050
or through your driver.

26
00:00:58.050 --> 00:01:00.510
So operations can be run
using the MongoDB query

27
00:01:00.510 --> 00:01:03.750
language, or MQL, which
includes most but not

28
00:01:03.750 --> 00:01:05.940
all standard server commands.

29
00:01:05.940 --> 00:01:08.880
In particular, the data
lake is a read-only service,

30
00:01:08.880 --> 00:01:11.730
though we will be adding support
for the dollar out aggregation

31
00:01:11.730 --> 00:01:14.090
stage sometime in the future.

32
00:01:14.090 --> 00:01:16.090
MQL operations can
be run in parallel

33
00:01:16.090 --> 00:01:18.880
to enhance performance for
large and complex queries.

34
00:01:18.880 --> 00:01:22.150
However, the data lake is meant
for analytics-type workloads

35
00:01:22.150 --> 00:01:24.220
and is not intended for
day-to-day operational

36
00:01:24.220 --> 00:01:25.270
workloads.

37
00:01:25.270 --> 00:01:27.490
The data lake supports many
different file formats,

38
00:01:27.490 --> 00:01:32.890
such as JSON, BSON, CSV,
TSV, Avro, and Parquet.

39
00:01:32.890 --> 00:01:35.140
You can specify the
file format, or it

40
00:01:35.140 --> 00:01:37.170
will be detected automatically.

41
00:01:37.170 --> 00:01:41.920
An AWS IAM role is required
to access the S3 data.

42
00:01:41.920 --> 00:01:44.380
This rule is created
using the AWS command line

43
00:01:44.380 --> 00:01:46.750
interface or the AWS console.

44
00:01:46.750 --> 00:01:48.280
You can then assign
a role policy

45
00:01:48.280 --> 00:01:51.370
to this role that gives Atlas
data lake read-only access

46
00:01:51.370 --> 00:01:53.290
to your S3 buckets.

47
00:01:53.290 --> 00:01:55.570
Using this approach, you
control what buckets are

48
00:01:55.570 --> 00:01:57.310
exposed to the Atlas data lake.

49
00:01:57.310 --> 00:01:59.740
Also, should you decide
to terminate access

50
00:01:59.740 --> 00:02:02.230
to the archived data, you
can remove or restrict

51
00:02:02.230 --> 00:02:04.990
a configured AWS role
whenever you want.

52
00:02:04.990 --> 00:02:07.300
Instructions are
provided in the Atlas UI

53
00:02:07.300 --> 00:02:10.780
during the data lake setup
to help you create this role.

54
00:02:10.780 --> 00:02:14.060
You can find more detail on IAM
role management in the AWS User

55
00:02:14.060 --> 00:02:14.560
Guide.

56
00:02:14.560 --> 00:02:17.640
And we'll attach a link to
that in the lecture notes.

57
00:02:17.640 --> 00:02:20.100
So a few things to consider
about using the Atlas data

58
00:02:20.100 --> 00:02:22.680
lake, as this is a
very new feature.

59
00:02:22.680 --> 00:02:25.840
Currently, the maximum
cursor size is 1 gigabyte.

60
00:02:25.840 --> 00:02:28.860
So if the query returns more
than 1 gigabyte of data,

61
00:02:28.860 --> 00:02:31.320
the result set will be
truncated down to 1 gigabyte

62
00:02:31.320 --> 00:02:32.960
and then returned.

63
00:02:32.960 --> 00:02:35.090
So as we mentioned before,
the Atlas data lake

64
00:02:35.090 --> 00:02:37.250
does not currently
support dollar out.

65
00:02:37.250 --> 00:02:39.050
There are a few other
aggregation pipeline

66
00:02:39.050 --> 00:02:42.260
stages that are not supported
by the data lake at this time.

67
00:02:42.260 --> 00:02:45.650
Those stages are collection
stats, geoNear, graphLookup,

68
00:02:45.650 --> 00:02:48.530
indexStats, and listSessions.

69
00:02:48.530 --> 00:02:50.780
In particular, the
collection stats command

70
00:02:50.780 --> 00:02:52.730
omits some fields, as
they are either not

71
00:02:52.730 --> 00:02:54.620
applicable to the data
lake, or they would

72
00:02:54.620 --> 00:02:57.050
be too expensive to compute.

73
00:02:57.050 --> 00:02:59.450
And lastly, the Atlas
data lake supports

74
00:02:59.450 --> 00:03:03.080
all of Parquet's base types,
except the deprecated INT96

75
00:03:03.080 --> 00:03:04.440
type.

76
00:03:04.440 --> 00:03:05.960
So now let's recap.

77
00:03:05.960 --> 00:03:08.060
The Atlas data lake
allows you to run queries

78
00:03:08.060 --> 00:03:10.820
against data stored in S3
buckets through the Mongo

79
00:03:10.820 --> 00:03:14.380
Shell, your driver, or
through MongoDB Compass.

80
00:03:14.380 --> 00:03:16.925
The data lake is integrated
with MongoDB Atlas,

81
00:03:16.925 --> 00:03:18.800
so you can build data
lakes through the Atlas

82
00:03:18.800 --> 00:03:21.800
UI, similar to building
an Atlas cluster.

83
00:03:21.800 --> 00:03:23.820
There's no
infrastructure required.

84
00:03:23.820 --> 00:03:27.020
You just need an AWS IAM
user with read-only access

85
00:03:27.020 --> 00:03:29.000
to the S3 bucket.

86
00:03:29.000 --> 00:03:31.730
The stored configuration
allows you to flexibly define

87
00:03:31.730 --> 00:03:35.910
how your S3 data is mapped
to databases and collections.

88
00:03:35.910 --> 00:03:38.700
Our Atlas data lake tutorial
guides you through the data

89
00:03:38.700 --> 00:03:40.590
lake setup process.

90
00:03:40.590 --> 00:03:43.710
There are steps in the procedure
on how to set up a data lake,

91
00:03:43.710 --> 00:03:46.110
and it'll provide access
to a public S3 bucket

92
00:03:46.110 --> 00:03:48.660
with sample commands to
help you create named spaces

93
00:03:48.660 --> 00:03:50.730
and run MQL queries.

94
00:03:50.730 --> 00:03:52.950
We'll also provide a link
to this documentation page,

95
00:03:52.950 --> 00:03:56.450
as well as some other helpful
ones in the lecture notes.