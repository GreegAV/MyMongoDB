
1
00:00:00.000 --> 00:00:00.500


2
00:00:00.500 --> 00:00:04.770
Another very useful
utility stage is $sample.

3
00:00:04.770 --> 00:00:08.820
$sample will select a
set of random documents

4
00:00:08.820 --> 00:00:13.090
from a collection
in one of two ways.

5
00:00:13.090 --> 00:00:17.190
The first method is if the
size that we are specifying,

6
00:00:17.190 --> 00:00:21.900
the N number of documents
that we want our sample to be

7
00:00:21.900 --> 00:00:25.200
looking like, if
it's less than 5%

8
00:00:25.200 --> 00:00:28.170
of the number of documents
in our source collection,

9
00:00:28.170 --> 00:00:33.420
and the source collection
has more than 100 documents,

10
00:00:33.420 --> 00:00:39.600
and $sample is the first stage,
then a pseudo-random cursor

11
00:00:39.600 --> 00:00:44.490
will select the specific number
of documents to be passed on.

12
00:00:44.490 --> 00:00:46.620
If all other
conditions, and let's

13
00:00:46.620 --> 00:00:50.190
recap them very quickly,
if N is more than 5%

14
00:00:50.190 --> 00:00:55.740
or the source collection
has less than 100 documents,

15
00:00:55.740 --> 00:01:01.020
or if sample is not
the first stage, if any

16
00:01:01.020 --> 00:01:06.450
of these conditions does
not apply, then what is done

17
00:01:06.450 --> 00:01:10.350
is a in-memory random
sort and select

18
00:01:10.350 --> 00:01:14.970
the specific number of documents
that we specify as the size.

19
00:01:14.970 --> 00:01:20.490
Now this sort will be subjected
to the same memory restrictions

20
00:01:20.490 --> 00:01:25.050
as the sort stage of our
aggregation pipeline.

21
00:01:25.050 --> 00:01:27.060
So let's see some
of this in action.

22
00:01:27.060 --> 00:01:32.220
In my database, I will have
a NYC facilities collection.

23
00:01:32.220 --> 00:01:36.030
This collection contains
more than 100 documents.

24
00:01:36.030 --> 00:01:40.710
The sample size
is greater than 5%

25
00:01:40.710 --> 00:01:44.730
of the total amount
of documents.

26
00:01:44.730 --> 00:01:50.730
And the sample stage is
the first of my pipeline.

27
00:01:50.730 --> 00:01:57.510
Therefore, the pseudo-random
operation will apply.

28
00:01:57.510 --> 00:01:59.850
When I run this
pipeline, we can see

29
00:01:59.850 --> 00:02:06.670
that we got randomly selected
documents from our collection.

30
00:02:06.670 --> 00:02:12.930
Now $sample is very useful when
working with large collections.

31
00:02:12.930 --> 00:02:17.040
And we only want a limited
amount of documents

32
00:02:17.040 --> 00:02:18.420
to operate with.

33
00:02:18.420 --> 00:02:21.240
They can be useful to
do an initial analysis

34
00:02:21.240 --> 00:02:24.880
or to do some sampling on
the result set that we might

35
00:02:24.880 --> 00:02:27.090
be interested to work with.

36
00:02:27.090 --> 00:02:28.830
It can be used to
fetch documents

37
00:02:28.830 --> 00:02:31.930
in a random fashion for
features such as random user

38
00:02:31.930 --> 00:02:34.080
search in a
collection, or when we

39
00:02:34.080 --> 00:02:38.220
want to seed some random
object for some computation,

40
00:02:38.220 --> 00:02:42.000
or when we want aleatory
data for our data set.

41
00:02:42.000 --> 00:02:45.500
And this is all we have
for you on $sample.